{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fNeXLdf_ekmq"
      },
      "outputs": [],
      "source": [
        "import torch,sys,os\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzq0Y5WX2-NU"
      },
      "source": [
        "# Self reproduce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRXGP-qt3A_E"
      },
      "outputs": [],
      "source": [
        "from typing import Optional,Callable\n",
        "\n",
        "class GoogLeNet_v1(nn.Module):\n",
        "    def __init__(self,num_classes=1000,init_weights=False):\n",
        "        super(GoogLeNet_v1,self).__init__()\n",
        "\n",
        "        self.Conv = nn.Sequential(\n",
        "            # 使用7x7的卷积核（滑动步长2，padding为3），64通道，输出为112x112x64，卷积后进行ReLU操作\n",
        "            nn.Conv2d(kernel_size=7,stride=2,padding=3,out_channels=64,in_channels=3), \n",
        "            nn.ReLU(),\n",
        "\n",
        "            # 经过3x3的max pooling（步长为2），输出为((112 - 3+1)/2)+1=56，即56x56x64，再进行ReLU操作\n",
        "            nn.MaxPool2d(stride=2,kernel_size=3),\n",
        "            nn.ReLU(),\n",
        "            nn.LocalResponseNorm(size=5, k=2, alpha=1e-4, beta=0.75),\n",
        "\n",
        "            # 使用3x3的卷积核（滑动步长为1，padding为1），192通道，输出为56x56x192，卷积后进行ReLU操作\n",
        "\n",
        "            nn.Conv2d(in_channels=64,out_channels=64,kernel_size=1),\n",
        "            nn.LocalResponseNorm(size=5, k=2, alpha=1e-4, beta=0.75),\n",
        "            nn.Conv2d(in_channels=64,out_channels=192,kernel_size=3,stride=1),\n",
        "            nn.LocalResponseNorm(size=5, k=2, alpha=1e-4, beta=0.75),\n",
        "            nn.ReLU(),\n",
        "            # 经过3x3的max pooling（步长为2），输出为((56 - 3+1)/2)+1=28，即28x28x192，再进行ReLU操作\n",
        "            nn.MaxPool2d(kernel_size=3,stride=2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.Inception_3a = Inception(in_channels=64,ch1x1=1)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x= self.Conv(x)\n",
        "        y\n",
        "        return y\n",
        "\n",
        "\n",
        "class Inception(nn.Module):\n",
        "    def __init__(self,\n",
        "        in_channels: int,\n",
        "        ch1x1: int,\n",
        "        ch3x3red: int,\n",
        "        ch3x3: int,\n",
        "        ch5x5red: int,\n",
        "        ch5x5: int,\n",
        "        pool_proj: int,\n",
        "        conv_block: Optional[Callable[..., nn.Module]] = None,) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nk2GUtd324J5"
      },
      "source": [
        "# Pytorch Version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Na3tP_1Jekmy",
        "outputId": "114d18c2-3381-4d00-f6aa-2c3e1f765e66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji2pJ5hz3Uwi"
      },
      "source": [
        " # STL10 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IP9kN1l9ekm1",
        "outputId": "7ccccad9-fe45-4979-e4b4-0fc240534bbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "_IncompatibleKeys(missing_keys=['Classifier.0.weight', 'Classifier.0.bias', 'Classifier.3.weight', 'Classifier.3.bias', 'Classifier.6.weight', 'Classifier.6.bias'], unexpected_keys=['classifier.0.weight', 'classifier.0.bias', 'classifier.3.weight', 'classifier.3.bias', 'classifier.6.weight', 'classifier.6.bias'])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transform=transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5,),(0.5,0.5,0.5)),\n",
        "\ttransforms.Resize([224, 224])\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Download training data from open datasets.\n",
        "train_set = datasets.STL10(\n",
        "    root=\"~/data/STL10/\",\n",
        "    split ='train',\n",
        "    download=True,\n",
        "    transform=transform, # transform,\n",
        ")\n",
        "trainloader=torch.utils.data.DataLoader(\n",
        "\ttrain_set,\n",
        "\tbatch_size=60,\n",
        "\tshuffle=True,\n",
        "\tpin_memory=True,\n",
        "    num_workers=8\n",
        "\t)\n",
        "\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_set = datasets.STL10(\n",
        "    root=\"~/data/STL10/\",\n",
        "    split ='test',\n",
        "    download=True,\n",
        "    transform=transform, # transform,\n",
        ")\n",
        "testloader=torch.utils.data.DataLoader(\n",
        "\ttest_set,\n",
        "\tbatch_size=60,\n",
        "\tshuffle=False,\n",
        "    pin_memory=True,\n",
        "    num_workers=8\n",
        "\t)\n",
        "\n",
        "# test_data_iter=iter(testloader)\n",
        "# test_image,test_label=test_data_iter.next()\n",
        "test_num  = len(test_set)\n",
        "train_steps = len(trainloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92UjNaPq3hF6"
      },
      "source": [
        "# loss and optimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bigYof8sekm3"
      },
      "outputs": [],
      "source": [
        "# 定义一个损失函数\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# 定义一个优化器\n",
        "# optimizer = torch.optim.Adam(model.parameters(),lr=0.005)\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
        "# optimizer = torch.optim.SGD(model.parameters(),lr=0.001)\n",
        "\n",
        "epochs = 40\n",
        "\n",
        "save_path= './VGG16.pth'\n",
        "best_acc = 0.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcasfhHN4rGn"
      },
      "source": [
        "# Transfer learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B16aNteM4mlH"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# from torchvision import models\n",
        "# pretrained_VGG16 = models.vgg16(pretrained=True)\n",
        "# torch.save(pretrained_VGG16.state_dict(), save_path)\n",
        "\n",
        "model = VGG16(num_classes=10, init_weights=True).to(device)\n",
        "model.load_state_dict(torch.load('./VGG16.pth'),strict=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06G0oqoc3p_d"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JpLHjMKRekm4",
        "outputId": "1691b337-5917-4846-c943-2ecf27a4700d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train epoch[1/40] loss:1.502: 100%|██████████| 84/84 [01:12<00:00,  1.16it/s]\n",
            "100%|██████████| 134/134 [00:38<00:00,  3.49it/s]\n",
            "[epoch 1] train_loss: 2.189  val_accuracy: 0.696\n",
            "train epoch[2/40] loss:1.107: 100%|██████████| 84/84 [01:10<00:00,  1.19it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.27it/s]\n",
            "[epoch 2] train_loss: 1.365  val_accuracy: 0.832\n",
            "train epoch[3/40] loss:0.932: 100%|██████████| 84/84 [01:09<00:00,  1.20it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.33it/s]\n",
            "[epoch 3] train_loss: 0.854  val_accuracy: 0.872\n",
            "train epoch[4/40] loss:0.429: 100%|██████████| 84/84 [01:09<00:00,  1.21it/s]\n",
            "100%|██████████| 134/134 [00:39<00:00,  3.38it/s]\n",
            "[epoch 4] train_loss: 0.623  val_accuracy: 0.902\n",
            "train epoch[5/40] loss:0.225: 100%|██████████| 84/84 [01:10<00:00,  1.19it/s]\n",
            "100%|██████████| 134/134 [00:41<00:00,  3.27it/s]\n",
            "[epoch 5] train_loss: 0.502  val_accuracy: 0.913\n",
            "train epoch[6/40] loss:0.206: 100%|██████████| 84/84 [01:10<00:00,  1.20it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.30it/s]\n",
            "[epoch 6] train_loss: 0.390  val_accuracy: 0.920\n",
            "train epoch[7/40] loss:0.293: 100%|██████████| 84/84 [01:10<00:00,  1.20it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.31it/s]\n",
            "[epoch 7] train_loss: 0.339  val_accuracy: 0.924\n",
            "train epoch[8/40] loss:0.142: 100%|██████████| 84/84 [01:10<00:00,  1.20it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.30it/s]\n",
            "[epoch 8] train_loss: 0.281  val_accuracy: 0.925\n",
            "train epoch[9/40] loss:0.233: 100%|██████████| 84/84 [01:09<00:00,  1.20it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.33it/s]\n",
            "[epoch 9] train_loss: 0.267  val_accuracy: 0.930\n",
            "train epoch[10/40] loss:0.200: 100%|██████████| 84/84 [01:10<00:00,  1.19it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.33it/s]\n",
            "[epoch 10] train_loss: 0.233  val_accuracy: 0.933\n",
            "train epoch[11/40] loss:0.330: 100%|██████████| 84/84 [01:10<00:00,  1.20it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.33it/s]\n",
            "[epoch 11] train_loss: 0.208  val_accuracy: 0.934\n",
            "train epoch[12/40] loss:0.107: 100%|██████████| 84/84 [01:10<00:00,  1.20it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.34it/s]\n",
            "[epoch 12] train_loss: 0.177  val_accuracy: 0.937\n",
            "train epoch[13/40] loss:0.100: 100%|██████████| 84/84 [01:10<00:00,  1.19it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.30it/s]\n",
            "[epoch 13] train_loss: 0.157  val_accuracy: 0.937\n",
            "train epoch[14/40] loss:0.261: 100%|██████████| 84/84 [01:10<00:00,  1.19it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.33it/s]\n",
            "[epoch 14] train_loss: 0.153  val_accuracy: 0.939\n",
            "train epoch[15/40] loss:0.150: 100%|██████████| 84/84 [01:10<00:00,  1.20it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.32it/s]\n",
            "[epoch 15] train_loss: 0.138  val_accuracy: 0.939\n",
            "train epoch[16/40] loss:0.037: 100%|██████████| 84/84 [01:10<00:00,  1.18it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.33it/s]\n",
            "[epoch 16] train_loss: 0.120  val_accuracy: 0.942\n",
            "train epoch[17/40] loss:0.138: 100%|██████████| 84/84 [01:10<00:00,  1.20it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.32it/s]\n",
            "[epoch 17] train_loss: 0.122  val_accuracy: 0.940\n",
            "train epoch[18/40] loss:0.118: 100%|██████████| 84/84 [01:10<00:00,  1.20it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.29it/s]\n",
            "[epoch 18] train_loss: 0.113  val_accuracy: 0.941\n",
            "train epoch[19/40] loss:0.128: 100%|██████████| 84/84 [01:09<00:00,  1.20it/s]\n",
            "100%|██████████| 134/134 [00:41<00:00,  3.24it/s]\n",
            "[epoch 19] train_loss: 0.093  val_accuracy: 0.940\n",
            "train epoch[20/40] loss:0.047: 100%|██████████| 84/84 [01:10<00:00,  1.20it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.31it/s]\n",
            "[epoch 20] train_loss: 0.086  val_accuracy: 0.941\n",
            "train epoch[21/40] loss:0.039: 100%|██████████| 84/84 [01:09<00:00,  1.20it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.32it/s]\n",
            "[epoch 21] train_loss: 0.074  val_accuracy: 0.942\n",
            "train epoch[22/40] loss:0.223: 100%|██████████| 84/84 [01:10<00:00,  1.20it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.33it/s]\n",
            "[epoch 22] train_loss: 0.070  val_accuracy: 0.939\n",
            "train epoch[23/40] loss:0.031: 100%|██████████| 84/84 [01:10<00:00,  1.20it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.32it/s]\n",
            "[epoch 23] train_loss: 0.065  val_accuracy: 0.943\n",
            "train epoch[24/40] loss:0.330: 100%|██████████| 84/84 [01:10<00:00,  1.20it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.28it/s]\n",
            "[epoch 24] train_loss: 0.068  val_accuracy: 0.943\n",
            "train epoch[25/40] loss:0.010: 100%|██████████| 84/84 [01:10<00:00,  1.19it/s]\n",
            "100%|██████████| 134/134 [00:41<00:00,  3.27it/s]\n",
            "[epoch 25] train_loss: 0.064  val_accuracy: 0.941\n",
            "train epoch[26/40] loss:0.012: 100%|██████████| 84/84 [01:10<00:00,  1.20it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.30it/s]\n",
            "[epoch 26] train_loss: 0.061  val_accuracy: 0.943\n",
            "train epoch[27/40] loss:0.027: 100%|██████████| 84/84 [01:10<00:00,  1.19it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.28it/s]\n",
            "[epoch 27] train_loss: 0.048  val_accuracy: 0.943\n",
            "train epoch[28/40] loss:0.006: 100%|██████████| 84/84 [01:10<00:00,  1.20it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.30it/s]\n",
            "[epoch 28] train_loss: 0.047  val_accuracy: 0.942\n",
            "train epoch[29/40] loss:0.050:  10%|▉         | 8/84 [00:09<01:30,  1.20s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-786ba45e21bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;31m# print statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mtrain_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"train epoch[{}/{}] loss:{:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "        # train\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        train_bar = tqdm(trainloader, file=sys.stdout)\n",
        "        for step, data in enumerate(train_bar):\n",
        "            images, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images.to(device))\n",
        "            loss = loss_fn(outputs, labels.to(device))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            train_bar.desc = \"train epoch[{}/{}] loss:{:.3f}\".format(epoch + 1,epochs,loss)\n",
        "\n",
        "        # validate\n",
        "        model.eval()\n",
        "        acc = 0.0  # accumulate accurate number / epoch\n",
        "        with torch.no_grad():\n",
        "            val_bar = tqdm(testloader, file=sys.stdout) # show progress\n",
        "            for val_data in val_bar:\n",
        "                val_images, val_labels = val_data\n",
        "                outputs = model(val_images.to(device))\n",
        "                predict_y = torch.max(outputs, dim=1)[1]\n",
        "                acc += torch.eq(predict_y, val_labels.to(device)).sum().item()\n",
        "\n",
        "        val_accurate = acc / test_num\n",
        "        print('[epoch %d] train_loss: %.3f  val_accuracy: %.3f' %\n",
        "              (epoch + 1, running_loss / train_steps, val_accurate))\n",
        "\n",
        "        if val_accurate > best_acc:\n",
        "            best_acc = val_accurate\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "\n",
        "print('Finished Training')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('ryan': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "e70b0d8493b7f59e214a43b868537ebeafe12cb89daa279090c57b89e62c1c99"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
