{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch,sys,os\n",
    "from torch import nn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "\n",
    "from torchvision.transforms import ToTensor,transforms\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST = datasets.MNIST('~/data/MNIST/')\n",
    "# data_loader = torch.utils.data.DataLoader(MNIST,\n",
    "#                                           batch_size=4,\n",
    "#                                           shuffle=True,\n",
    "#                                           num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mode\n",
    "from turtle import forward\n",
    "\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self, num_classes, init_weights=False) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # self.conv1 = nn.Conv2d(in_channels=1,kernel_size=5,out_channels=6,stride=1)\n",
    "        # self.maxPool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "\n",
    "        # self.conv2 = nn.Conv2d(in_channels=6,kernel_size=5,out_channels=16)\n",
    "\n",
    "        # self.flattern = nn.Flatten()\n",
    "\n",
    "        # self.fc1 = nn.Linear(in_features=120,out_features=84)\n",
    "        # self.fc2 = nn.Linear(in_features=84,out_features=10)\n",
    "\n",
    "        # self.out = nn.Softmax(dim=10)\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, stride=1),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=400,out_features=120),\n",
    "            nn.Linear(in_features=120,out_features=84),\n",
    "            nn.Linear(in_features=84,out_features=num_classes)\n",
    "            )\n",
    "            \n",
    "        if init_weights:\n",
    "            self.weight_init()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        y = self.out(x)\n",
    "        return y\n",
    "\n",
    "    def weight_init(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='tanh')\n",
    "                # nn.init.normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet(num_classes=10,init_weights=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): Tanh()\n",
      "    (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): Tanh()\n",
      "    (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  )\n",
      "  (out): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (3): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# transform=transforms.Compose(\n",
    "#     [transforms.ToTensor(),\n",
    "#      transforms.Normalize((0.5,0.5,0.5,),(0.5,0.5,0.5))\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# Download training data from open datasets.\n",
    "train_set = datasets.CIFAR10(\n",
    "    root=\"~/data/CIFAR10/\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor() # transform,\n",
    ")\n",
    "trainloader=torch.utils.data.DataLoader(\n",
    "\ttrain_set,\n",
    "\tbatch_size=36,\n",
    "\tshuffle=True,\n",
    "\tnum_workers=0\n",
    "\t)\n",
    "\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_set = datasets.CIFAR10(\n",
    "    root=\"~/data/CIFAR10/\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor() # transform,\n",
    ")\n",
    "testloader=torch.utils.data.DataLoader(\n",
    "\ttest_set,\n",
    "\tbatch_size=10000,\n",
    "\tshuffle=False,\n",
    "\tnum_workers=0\n",
    "\t)\n",
    "\n",
    "test_data_iter=iter(testloader)\n",
    "test_image,test_label=test_data_iter.next()\n",
    "test_num  = len(test_set)\n",
    "\n",
    "train_steps = len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个损失函数\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 定义一个优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "epochs = 40\n",
    "\n",
    "save_path= './LeNet.pth'\n",
    "best_acc = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch[1/40] loss:1.383: 100%|██████████| 1389/1389 [00:15<00:00, 91.39it/s] \n",
      "100%|██████████| 1/1 [00:01<00:00,  2.00s/it]\n",
      "[epoch 1] train_loss: 1.874  val_accuracy: 0.366\n",
      "train epoch[2/40] loss:1.707: 100%|██████████| 1389/1389 [00:13<00:00, 105.48it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.12s/it]\n",
      "[epoch 2] train_loss: 1.663  val_accuracy: 0.432\n",
      "train epoch[3/40] loss:1.648: 100%|██████████| 1389/1389 [00:13<00:00, 105.64it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.14s/it]\n",
      "[epoch 3] train_loss: 1.546  val_accuracy: 0.444\n",
      "train epoch[4/40] loss:1.683: 100%|██████████| 1389/1389 [00:12<00:00, 107.81it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.11s/it]\n",
      "[epoch 4] train_loss: 1.485  val_accuracy: 0.460\n",
      "train epoch[5/40] loss:1.545: 100%|██████████| 1389/1389 [00:13<00:00, 105.29it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n",
      "[epoch 5] train_loss: 1.449  val_accuracy: 0.493\n",
      "train epoch[6/40] loss:1.085: 100%|██████████| 1389/1389 [00:12<00:00, 109.37it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.13s/it]\n",
      "[epoch 6] train_loss: 1.416  val_accuracy: 0.495\n",
      "train epoch[7/40] loss:1.328: 100%|██████████| 1389/1389 [00:13<00:00, 105.61it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.16s/it]\n",
      "[epoch 7] train_loss: 1.394  val_accuracy: 0.502\n",
      "train epoch[8/40] loss:1.217: 100%|██████████| 1389/1389 [00:13<00:00, 100.37it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n",
      "[epoch 8] train_loss: 1.373  val_accuracy: 0.511\n",
      "train epoch[9/40] loss:1.394: 100%|██████████| 1389/1389 [00:13<00:00, 102.65it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.12s/it]\n",
      "[epoch 9] train_loss: 1.353  val_accuracy: 0.511\n",
      "train epoch[10/40] loss:1.706: 100%|██████████| 1389/1389 [00:13<00:00, 102.95it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.12s/it]\n",
      "[epoch 10] train_loss: 1.334  val_accuracy: 0.510\n",
      "train epoch[11/40] loss:1.398: 100%|██████████| 1389/1389 [00:13<00:00, 106.40it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.16s/it]\n",
      "[epoch 11] train_loss: 1.322  val_accuracy: 0.523\n",
      "train epoch[12/40] loss:1.244: 100%|██████████| 1389/1389 [00:13<00:00, 104.45it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.13s/it]\n",
      "[epoch 12] train_loss: 1.306  val_accuracy: 0.530\n",
      "train epoch[13/40] loss:1.622: 100%|██████████| 1389/1389 [00:13<00:00, 106.72it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.11s/it]\n",
      "[epoch 13] train_loss: 1.290  val_accuracy: 0.523\n",
      "train epoch[14/40] loss:1.197: 100%|██████████| 1389/1389 [00:12<00:00, 107.91it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.12s/it]\n",
      "[epoch 14] train_loss: 1.279  val_accuracy: 0.527\n",
      "train epoch[15/40] loss:1.493: 100%|██████████| 1389/1389 [00:13<00:00, 104.99it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.17s/it]\n",
      "[epoch 15] train_loss: 1.268  val_accuracy: 0.540\n",
      "train epoch[16/40] loss:0.874: 100%|██████████| 1389/1389 [00:13<00:00, 104.79it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.12s/it]\n",
      "[epoch 16] train_loss: 1.253  val_accuracy: 0.536\n",
      "train epoch[17/40] loss:1.194: 100%|██████████| 1389/1389 [00:12<00:00, 107.10it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.11s/it]\n",
      "[epoch 17] train_loss: 1.242  val_accuracy: 0.546\n",
      "train epoch[18/40] loss:1.393: 100%|██████████| 1389/1389 [00:13<00:00, 104.27it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n",
      "[epoch 18] train_loss: 1.230  val_accuracy: 0.548\n",
      "train epoch[19/40] loss:0.999: 100%|██████████| 1389/1389 [00:13<00:00, 103.03it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.11s/it]\n",
      "[epoch 19] train_loss: 1.223  val_accuracy: 0.555\n",
      "train epoch[20/40] loss:1.170: 100%|██████████| 1389/1389 [00:12<00:00, 108.35it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.11s/it]\n",
      "[epoch 20] train_loss: 1.209  val_accuracy: 0.551\n",
      "train epoch[21/40] loss:1.134: 100%|██████████| 1389/1389 [00:12<00:00, 107.65it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.16s/it]\n",
      "[epoch 21] train_loss: 1.202  val_accuracy: 0.559\n",
      "train epoch[22/40] loss:1.258: 100%|██████████| 1389/1389 [00:12<00:00, 108.43it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.11s/it]\n",
      "[epoch 22] train_loss: 1.193  val_accuracy: 0.557\n",
      "train epoch[23/40] loss:1.066: 100%|██████████| 1389/1389 [00:12<00:00, 107.53it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.16s/it]\n",
      "[epoch 23] train_loss: 1.182  val_accuracy: 0.572\n",
      "train epoch[24/40] loss:1.180: 100%|██████████| 1389/1389 [00:13<00:00, 103.18it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.14s/it]\n",
      "[epoch 24] train_loss: 1.173  val_accuracy: 0.570\n",
      "train epoch[25/40] loss:1.278: 100%|██████████| 1389/1389 [00:13<00:00, 105.36it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.16s/it]\n",
      "[epoch 25] train_loss: 1.169  val_accuracy: 0.565\n",
      "train epoch[26/40] loss:1.060: 100%|██████████| 1389/1389 [00:12<00:00, 107.21it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.14s/it]\n",
      "[epoch 26] train_loss: 1.163  val_accuracy: 0.573\n",
      "train epoch[27/40] loss:1.219: 100%|██████████| 1389/1389 [00:12<00:00, 107.23it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.13s/it]\n",
      "[epoch 27] train_loss: 1.153  val_accuracy: 0.583\n",
      "train epoch[28/40] loss:1.444: 100%|██████████| 1389/1389 [00:13<00:00, 105.45it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n",
      "[epoch 28] train_loss: 1.147  val_accuracy: 0.583\n",
      "train epoch[29/40] loss:1.131: 100%|██████████| 1389/1389 [00:12<00:00, 107.12it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n",
      "[epoch 29] train_loss: 1.142  val_accuracy: 0.584\n",
      "train epoch[30/40] loss:1.387: 100%|██████████| 1389/1389 [00:13<00:00, 101.44it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.11s/it]\n",
      "[epoch 30] train_loss: 1.136  val_accuracy: 0.571\n",
      "train epoch[31/40] loss:1.161: 100%|██████████| 1389/1389 [00:13<00:00, 104.11it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.17s/it]\n",
      "[epoch 31] train_loss: 1.130  val_accuracy: 0.582\n",
      "train epoch[32/40] loss:1.018: 100%|██████████| 1389/1389 [00:13<00:00, 101.41it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.15s/it]\n",
      "[epoch 32] train_loss: 1.126  val_accuracy: 0.590\n",
      "train epoch[33/40] loss:1.047: 100%|██████████| 1389/1389 [00:13<00:00, 102.87it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.42s/it]\n",
      "[epoch 33] train_loss: 1.119  val_accuracy: 0.583\n",
      "train epoch[34/40] loss:0.948: 100%|██████████| 1389/1389 [00:13<00:00, 103.94it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.13s/it]\n",
      "[epoch 34] train_loss: 1.115  val_accuracy: 0.567\n",
      "train epoch[35/40] loss:1.146: 100%|██████████| 1389/1389 [00:13<00:00, 104.72it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.21s/it]\n",
      "[epoch 35] train_loss: 1.110  val_accuracy: 0.586\n",
      "train epoch[36/40] loss:0.912: 100%|██████████| 1389/1389 [00:12<00:00, 107.75it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.11s/it]\n",
      "[epoch 36] train_loss: 1.106  val_accuracy: 0.591\n",
      "train epoch[37/40] loss:1.412: 100%|██████████| 1389/1389 [00:12<00:00, 108.37it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.13s/it]\n",
      "[epoch 37] train_loss: 1.103  val_accuracy: 0.588\n",
      "train epoch[38/40] loss:0.956: 100%|██████████| 1389/1389 [00:12<00:00, 111.92it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.21s/it]\n",
      "[epoch 38] train_loss: 1.099  val_accuracy: 0.593\n",
      "train epoch[39/40] loss:0.925: 100%|██████████| 1389/1389 [00:12<00:00, 109.55it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.10s/it]\n",
      "[epoch 39] train_loss: 1.093  val_accuracy: 0.582\n",
      "train epoch[40/40] loss:1.398: 100%|██████████| 1389/1389 [00:13<00:00, 106.03it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.13s/it]\n",
      "[epoch 40] train_loss: 1.092  val_accuracy: 0.595\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "\n",
    "for epoch in range(epochs):\n",
    "        # train\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_bar = tqdm(trainloader, file=sys.stdout)\n",
    "        for step, data in enumerate(train_bar):\n",
    "            images, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images.to(device))\n",
    "            loss = loss_fn(outputs, labels.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            train_bar.desc = \"train epoch[{}/{}] loss:{:.3f}\".format(epoch + 1,\n",
    "                                                                     epochs,\n",
    "                                                                     loss)\n",
    "\n",
    "        # validate\n",
    "        model.eval()\n",
    "        acc = 0.0  # accumulate accurate number / epoch\n",
    "        with torch.no_grad():\n",
    "            val_bar = tqdm(testloader, file=sys.stdout) # show progress\n",
    "            for val_data in val_bar:\n",
    "                val_images, val_labels = val_data\n",
    "                outputs = model(val_images.to(device))\n",
    "                predict_y = torch.max(outputs, dim=1)[1]\n",
    "                acc += torch.eq(predict_y, val_labels.to(device)).sum().item()\n",
    "\n",
    "        val_accurate = acc / test_num\n",
    "        print('[epoch %d] train_loss: %.3f  val_accuracy: %.3f' %\n",
    "              (epoch + 1, running_loss / train_steps, val_accurate))\n",
    "\n",
    "        if val_accurate > best_acc:\n",
    "            best_acc = val_accurate\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeNet(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): Tanh()\n",
       "    (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): Tanh()\n",
       "    (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (out): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=400, out_features=120, bias=True)\n",
       "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
       "    (3): Linear(in_features=84, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_index = 5\n",
    "\n",
    "test_lab = test_label[test_index]\n",
    "test_img = val_images[test_index].unsqueeze(0)\n",
    "\n",
    "weights_path = \"./LeNet.pth\"\n",
    "assert os.path.exists(weights_path), \"file: '{}' dose not exist.\".format(weights_path)\n",
    "model.load_state_dict(torch.load(weights_path))\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5], device='cuda:0')\n",
      "tensor(6)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(test_img.to(device))\n",
    "    predict_y = torch.max(outputs, dim=1)[1]\n",
    "    print(predict_y)\n",
    "    print(test_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ryan': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e70b0d8493b7f59e214a43b868537ebeafe12cb89daa279090c57b89e62c1c99"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
