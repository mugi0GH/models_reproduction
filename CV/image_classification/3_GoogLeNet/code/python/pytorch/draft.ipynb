{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fNeXLdf_ekmq"
      },
      "outputs": [],
      "source": [
        "import torch,sys,os\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzq0Y5WX2-NU"
      },
      "source": [
        "# Self reproduce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRXGP-qt3A_E"
      },
      "outputs": [],
      "source": [
        "# class VGG16(nn.Module):\n",
        "#     def __init__(self,num_classes=102, init_weights=False):\n",
        "#         super(VGG16,self).__init__()\n",
        "\n",
        "#         self.conv1 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=3,out_channels=64,padding=1,kernel_size=3),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Conv2d(in_channels=64,out_channels=64,padding=1,kernel_size=3),\n",
        "#             nn.ReLU(),\n",
        "#             nn.MaxPool2d(stride=2,kernel_size=2)\n",
        "#         )\n",
        "\n",
        "#         self.conv2 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=64,out_channels=128,padding=1,kernel_size=3),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Conv2d(in_channels=128,out_channels=128,padding=1,kernel_size=3),\n",
        "#             nn.ReLU(),\n",
        "#             nn.MaxPool2d(stride=2,kernel_size=2)\n",
        "#         )\n",
        "\n",
        "#         self.conv3 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=128,out_channels=256,padding=1,kernel_size=3),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Conv2d(in_channels=256,out_channels=256,padding=1,kernel_size=3),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Conv2d(in_channels=256,out_channels=256,padding=1,kernel_size=3),\n",
        "#             nn.ReLU(),\n",
        "#             nn.MaxPool2d(stride=2,kernel_size=2)\n",
        "#         )\n",
        "\n",
        "#         self.conv4 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=256,out_channels=512,padding=1,kernel_size=3),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Conv2d(in_channels=512,out_channels=512,padding=1,kernel_size=3),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Conv2d(in_channels=512,out_channels=512,padding=1,kernel_size=3),\n",
        "#             nn.ReLU(),\n",
        "#             nn.MaxPool2d(stride=2,kernel_size=2)\n",
        "#         )\n",
        "\n",
        "#         self.conv5 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=512,out_channels=512,padding=1,kernel_size=3),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Conv2d(in_channels=512,out_channels=512,padding=1,kernel_size=3),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Conv2d(in_channels=512,out_channels=512,padding=1,kernel_size=3),\n",
        "#             nn.ReLU(),\n",
        "#             nn.MaxPool2d(stride=2,kernel_size=2)\n",
        "#         )        \n",
        "\n",
        "#         self.FC = nn.Sequential(\n",
        "#             nn.Linear(in_features=7*7*512,out_features=4096),\n",
        "#             nn.ReLU(True),\n",
        "#             nn.Dropout(p=0.5),\n",
        "#             nn.Linear(in_features=4096,out_features=4096),\n",
        "#             nn.ReLU(True),\n",
        "#             nn.Dropout(p=0.5),\n",
        "#             nn.Linear(in_features=4096,out_features=num_classes)\n",
        "#         )\n",
        "\n",
        "#         if init_weights:\n",
        "#             self._initialize_weights()\n",
        "\n",
        "#     def forward(self,x):\n",
        "#         x = self.conv1(x)\n",
        "#         # print(x.shape)\n",
        "#         x = self.conv2(x)\n",
        "#         # print(x.shape)\n",
        "#         x = self.conv3(x)\n",
        "#         # print(x.shape)\n",
        "#         x = self.conv4(x)\n",
        "#         # print(x.shape)\n",
        "#         x = self.conv5(x)\n",
        "#         # print(x.shape)\n",
        "#         x = torch.flatten(x, start_dim=1)\n",
        "#         y = self.FC(x)\n",
        "#         return y\n",
        "\n",
        "#     def _initialize_weights(self):\n",
        "#         for m in self.modules():\n",
        "#             if isinstance(m, nn.Conv2d):\n",
        "#                 # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "#                 nn.init.xavier_uniform_(m.weight)\n",
        "#                 if m.bias is not None:\n",
        "#                     nn.init.constant_(m.bias, 0)\n",
        "#             elif isinstance(m, nn.Linear):\n",
        "#                 nn.init.xavier_uniform_(m.weight)\n",
        "#                 # nn.init.normal_(m.weight, 0, 0.01)\n",
        "#                 nn.init.constant_(m.bias, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nk2GUtd324J5"
      },
      "source": [
        "# Pytorch Version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Na3tP_1Jekmy",
        "outputId": "114d18c2-3381-4d00-f6aa-2c3e1f765e66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "class VGG16(nn.Module):\n",
        "    def __init__(self,num_classes=102, init_weights=False):\n",
        "        super(VGG16,self).__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3,out_channels=64,padding=1,kernel_size=3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=64,out_channels=64,padding=1,kernel_size=3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(stride=2,kernel_size=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=64,out_channels=128,padding=1,kernel_size=3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=128,out_channels=128,padding=1,kernel_size=3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(stride=2,kernel_size=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=128,out_channels=256,padding=1,kernel_size=3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=256,out_channels=256,padding=1,kernel_size=3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=256,out_channels=256,padding=1,kernel_size=3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(stride=2,kernel_size=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=256,out_channels=512,padding=1,kernel_size=3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=512,out_channels=512,padding=1,kernel_size=3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=512,out_channels=512,padding=1,kernel_size=3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(stride=2,kernel_size=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=512,out_channels=512,padding=1,kernel_size=3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=512,out_channels=512,padding=1,kernel_size=3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=512,out_channels=512,padding=1,kernel_size=3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(stride=2,kernel_size=2)\n",
        "        )       \n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=7)\n",
        "        \n",
        "\n",
        "        self.Classifier = nn.Sequential(\n",
        "            nn.Linear(in_features=7*7*512,out_features=4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(in_features=4096,out_features=4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(in_features=4096,out_features=num_classes)\n",
        "        )\n",
        "\n",
        "        if init_weights:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.features(x)\n",
        "        # print(x.shape)\n",
        "        x = self.avgpool(x)\n",
        "\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        y = self.Classifier(x)\n",
        "        return y\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                # nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                # nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji2pJ5hz3Uwi"
      },
      "source": [
        " # STL10 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IP9kN1l9ekm1",
        "outputId": "7ccccad9-fe45-4979-e4b4-0fc240534bbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "_IncompatibleKeys(missing_keys=['Classifier.0.weight', 'Classifier.0.bias', 'Classifier.3.weight', 'Classifier.3.bias', 'Classifier.6.weight', 'Classifier.6.bias'], unexpected_keys=['classifier.0.weight', 'classifier.0.bias', 'classifier.3.weight', 'classifier.3.bias', 'classifier.6.weight', 'classifier.6.bias'])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transform=transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5,),(0.5,0.5,0.5)),\n",
        "\ttransforms.Resize([227, 227])\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Download training data from open datasets.\n",
        "train_set = datasets.STL10(\n",
        "    root=\"~/data/STL10/\",\n",
        "    split ='train',\n",
        "    download=True,\n",
        "    transform=transform, # transform,\n",
        ")\n",
        "trainloader=torch.utils.data.DataLoader(\n",
        "\ttrain_set,\n",
        "\tbatch_size=60,\n",
        "\tshuffle=True,\n",
        "\tpin_memory=True,\n",
        "    num_workers=8\n",
        "\t)\n",
        "\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_set = datasets.STL10(\n",
        "    root=\"~/data/STL10/\",\n",
        "    split ='test',\n",
        "    download=True,\n",
        "    transform=transform, # transform,\n",
        ")\n",
        "testloader=torch.utils.data.DataLoader(\n",
        "\ttest_set,\n",
        "\tbatch_size=60,\n",
        "\tshuffle=False,\n",
        "    pin_memory=True,\n",
        "    num_workers=8\n",
        "\t)\n",
        "\n",
        "# test_data_iter=iter(testloader)\n",
        "# test_image,test_label=test_data_iter.next()\n",
        "test_num  = len(test_set)\n",
        "train_steps = len(trainloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92UjNaPq3hF6"
      },
      "source": [
        "# loss and optimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bigYof8sekm3"
      },
      "outputs": [],
      "source": [
        "# 定义一个损失函数\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# 定义一个优化器\n",
        "# optimizer = torch.optim.Adam(model.parameters(),lr=0.005)\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
        "# optimizer = torch.optim.SGD(model.parameters(),lr=0.001)\n",
        "\n",
        "epochs = 40\n",
        "\n",
        "save_path= './GoogLeNet.pth'\n",
        "best_acc = 0.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcasfhHN4rGn"
      },
      "source": [
        "# Transfer learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B16aNteM4mlH"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# from torchvision import models\n",
        "# pretrained_googlenet = models.googlenet(pretrained=True)\n",
        "# torch.save(pretrained_googlenet.state_dict(), save_path)\n",
        "\n",
        "model = VGG16(num_classes=10, init_weights=True).to(device)\n",
        "model.load_state_dict(torch.load(save_path),strict=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06G0oqoc3p_d"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JpLHjMKRekm4",
        "outputId": "1691b337-5917-4846-c943-2ecf27a4700d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train epoch[1/40] loss:1.502: 100%|██████████| 84/84 [01:12<00:00,  1.16it/s]\n",
            "100%|██████████| 134/134 [00:38<00:00,  3.49it/s]\n",
            "[epoch 1] train_loss: 2.189  val_accuracy: 0.696\n",
            "train epoch[2/40] loss:1.107: 100%|██████████| 84/84 [01:10<00:00,  1.19it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.27it/s]\n",
            "[epoch 2] train_loss: 1.365  val_accuracy: 0.832\n",
            "train epoch[3/40] loss:0.932: 100%|██████████| 84/84 [01:09<00:00,  1.20it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.33it/s]\n",
            "[epoch 3] train_loss: 0.854  val_accuracy: 0.872\n",
            "train epoch[4/40] loss:0.429: 100%|██████████| 84/84 [01:09<00:00,  1.21it/s]\n",
            "100%|██████████| 134/134 [00:39<00:00,  3.38it/s]\n",
            "[epoch 4] train_loss: 0.623  val_accuracy: 0.902\n",
            "train epoch[5/40] loss:0.225: 100%|██████████| 84/84 [01:10<00:00,  1.19it/s]\n",
            "100%|██████████| 134/134 [00:41<00:00,  3.27it/s]\n",
            "[epoch 5] train_loss: 0.502  val_accuracy: 0.913\n",
            "train epoch[6/40] loss:0.206: 100%|██████████| 84/84 [01:10<00:00,  1.20it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.30it/s]\n",
            "[epoch 6] train_loss: 0.390  val_accuracy: 0.920\n",
            "train epoch[7/40] loss:0.293: 100%|██████████| 84/84 [01:10<00:00,  1.20it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.31it/s]\n",
            "[epoch 7] train_loss: 0.339  val_accuracy: 0.924\n",
            "train epoch[8/40] loss:0.142: 100%|██████████| 84/84 [01:10<00:00,  1.20it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.30it/s]\n",
            "[epoch 8] train_loss: 0.281  val_accuracy: 0.925\n",
            "train epoch[9/40] loss:0.233: 100%|██████████| 84/84 [01:09<00:00,  1.20it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.33it/s]\n",
            "[epoch 9] train_loss: 0.267  val_accuracy: 0.930\n",
            "train epoch[10/40] loss:0.200: 100%|██████████| 84/84 [01:10<00:00,  1.19it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.33it/s]\n",
            "[epoch 10] train_loss: 0.233  val_accuracy: 0.933\n",
            "train epoch[11/40] loss:0.330: 100%|██████████| 84/84 [01:10<00:00,  1.20it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.33it/s]\n",
            "[epoch 11] train_loss: 0.208  val_accuracy: 0.934\n",
            "train epoch[12/40] loss:0.107: 100%|██████████| 84/84 [01:10<00:00,  1.20it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.34it/s]\n",
            "[epoch 12] train_loss: 0.177  val_accuracy: 0.937\n",
            "train epoch[13/40] loss:0.100: 100%|██████████| 84/84 [01:10<00:00,  1.19it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.30it/s]\n",
            "[epoch 13] train_loss: 0.157  val_accuracy: 0.937\n",
            "train epoch[14/40] loss:0.261: 100%|██████████| 84/84 [01:10<00:00,  1.19it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.33it/s]\n",
            "[epoch 14] train_loss: 0.153  val_accuracy: 0.939\n",
            "train epoch[15/40] loss:0.150: 100%|██████████| 84/84 [01:10<00:00,  1.20it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.32it/s]\n",
            "[epoch 15] train_loss: 0.138  val_accuracy: 0.939\n",
            "train epoch[16/40] loss:0.037: 100%|██████████| 84/84 [01:10<00:00,  1.18it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.33it/s]\n",
            "[epoch 16] train_loss: 0.120  val_accuracy: 0.942\n",
            "train epoch[17/40] loss:0.138: 100%|██████████| 84/84 [01:10<00:00,  1.20it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.32it/s]\n",
            "[epoch 17] train_loss: 0.122  val_accuracy: 0.940\n",
            "train epoch[18/40] loss:0.118: 100%|██████████| 84/84 [01:10<00:00,  1.20it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.29it/s]\n",
            "[epoch 18] train_loss: 0.113  val_accuracy: 0.941\n",
            "train epoch[19/40] loss:0.128: 100%|██████████| 84/84 [01:09<00:00,  1.20it/s]\n",
            "100%|██████████| 134/134 [00:41<00:00,  3.24it/s]\n",
            "[epoch 19] train_loss: 0.093  val_accuracy: 0.940\n",
            "train epoch[20/40] loss:0.047: 100%|██████████| 84/84 [01:10<00:00,  1.20it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.31it/s]\n",
            "[epoch 20] train_loss: 0.086  val_accuracy: 0.941\n",
            "train epoch[21/40] loss:0.039: 100%|██████████| 84/84 [01:09<00:00,  1.20it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.32it/s]\n",
            "[epoch 21] train_loss: 0.074  val_accuracy: 0.942\n",
            "train epoch[22/40] loss:0.223: 100%|██████████| 84/84 [01:10<00:00,  1.20it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.33it/s]\n",
            "[epoch 22] train_loss: 0.070  val_accuracy: 0.939\n",
            "train epoch[23/40] loss:0.031: 100%|██████████| 84/84 [01:10<00:00,  1.20it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.32it/s]\n",
            "[epoch 23] train_loss: 0.065  val_accuracy: 0.943\n",
            "train epoch[24/40] loss:0.330: 100%|██████████| 84/84 [01:10<00:00,  1.20it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.28it/s]\n",
            "[epoch 24] train_loss: 0.068  val_accuracy: 0.943\n",
            "train epoch[25/40] loss:0.010: 100%|██████████| 84/84 [01:10<00:00,  1.19it/s]\n",
            "100%|██████████| 134/134 [00:41<00:00,  3.27it/s]\n",
            "[epoch 25] train_loss: 0.064  val_accuracy: 0.941\n",
            "train epoch[26/40] loss:0.012: 100%|██████████| 84/84 [01:10<00:00,  1.20it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.30it/s]\n",
            "[epoch 26] train_loss: 0.061  val_accuracy: 0.943\n",
            "train epoch[27/40] loss:0.027: 100%|██████████| 84/84 [01:10<00:00,  1.19it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.28it/s]\n",
            "[epoch 27] train_loss: 0.048  val_accuracy: 0.943\n",
            "train epoch[28/40] loss:0.006: 100%|██████████| 84/84 [01:10<00:00,  1.20it/s]\n",
            "100%|██████████| 134/134 [00:40<00:00,  3.30it/s]\n",
            "[epoch 28] train_loss: 0.047  val_accuracy: 0.942\n",
            "train epoch[29/40] loss:0.050:  10%|▉         | 8/84 [00:09<01:30,  1.20s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-786ba45e21bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;31m# print statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mtrain_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"train epoch[{}/{}] loss:{:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "        # train\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        train_bar = tqdm(trainloader, file=sys.stdout)\n",
        "        for step, data in enumerate(train_bar):\n",
        "            images, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images.to(device))\n",
        "            loss = loss_fn(outputs, labels.to(device))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            train_bar.desc = \"train epoch[{}/{}] loss:{:.3f}\".format(epoch + 1,epochs,loss)\n",
        "\n",
        "        # validate\n",
        "        model.eval()\n",
        "        acc = 0.0  # accumulate accurate number / epoch\n",
        "        with torch.no_grad():\n",
        "            val_bar = tqdm(testloader, file=sys.stdout) # show progress\n",
        "            for val_data in val_bar:\n",
        "                val_images, val_labels = val_data\n",
        "                outputs = model(val_images.to(device))\n",
        "                predict_y = torch.max(outputs, dim=1)[1]\n",
        "                acc += torch.eq(predict_y, val_labels.to(device)).sum().item()\n",
        "\n",
        "        val_accurate = acc / test_num\n",
        "        print('[epoch %d] train_loss: %.3f  val_accuracy: %.3f' %\n",
        "              (epoch + 1, running_loss / train_steps, val_accurate))\n",
        "\n",
        "        if val_accurate > best_acc:\n",
        "            best_acc = val_accurate\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "\n",
        "print('Finished Training')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('ryan': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "e70b0d8493b7f59e214a43b868537ebeafe12cb89daa279090c57b89e62c1c99"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
